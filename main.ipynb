{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c54ae28",
   "metadata": {},
   "source": [
    "# Catbot - Miao\n",
    "\n",
    "This notebook \n",
    "* Creates a database using ChromaDB\n",
    "* Creates a QA RAG to query the database\n",
    "* Generates a synthetic dataset\n",
    "* Evaluation the responses obtained from the RAG using Ragas metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e484d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_evaluation_models' from 'ragas.utils' (/Users/mariabader/Documents/catbot/.catbot/lib/python3.13/site-packages/ragas/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Synthetic data\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtestset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestsetGenerator\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_evaluation_models\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SingleTurnSample \n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_evaluation_models' from 'ragas.utils' (/Users/mariabader/Documents/catbot/.catbot/lib/python3.13/site-packages/ragas/utils.py)"
     ]
    }
   ],
   "source": [
    "# Database\n",
    "from catbot.database.downloader import download_and_chunk_wikipedia_articles\n",
    "from catbot.database.embedding import create_data_base\n",
    "\n",
    "# RAG\n",
    "from chromadb import PersistentClient\n",
    "from catbot.database.embedding import embedding_function\n",
    "from catbot.rag.basic_rag import BasicRAG\n",
    "\n",
    "# Synthetic data\n",
    "from ragas.testset import TestsetGenerator\n",
    "from catbot.utils import get_evaluation_models\n",
    "\n",
    "# Evaluation\n",
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from ragas.metrics import (\n",
    "    AnswerCorrectness,\n",
    "    AnswerSimilarity,\n",
    "    Faithfulness,\n",
    "    LLMContextRecall,\n",
    "    ResponseRelevancy,\n",
    "    SemanticSimilarity,\n",
    ")\n",
    "\n",
    "generator_llm, generator_embeddings = get_evaluation_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd27b9a",
   "metadata": {},
   "source": [
    "## Create the databse\n",
    "Download the articles under 'Cat' and chunk them according to their sections. Then, embedd the chunks and save them as a chroma collection.\n",
    "\n",
    "This only needs to run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1db191",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = download_and_chunk_wikipedia_articles(['Cat'])\n",
    "create_data_base(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094dd0d8",
   "metadata": {},
   "source": [
    "## Set up the QA RAG\n",
    "Load the locally stored database into a chroma collection. Provide it to the basic RAG - done. You can now query the rag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ac25d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is the history of cats?\n",
      "Response:  Cats have been domesticated for nearly 10,000 years. The oldest evidence of cats kept as pets is from the Mediterranean island of Cyprus, around 7500 BC. In the past, mostly in Egypt, people kept cats because they hunted and ate mice and rats. Ancient Egyptians worshipped cats as gods and often mummified them so they could be with their owners \"for all of eternity\". Cats started becoming pets during the time of the ancient Egyptians. Today, many people keep cats as pets, and some domestic cats live without human care as feral or stray cats.\n",
      "Contexts:\n",
      "-> Cat - History\n",
      "-> Cat - Introduction\n"
     ]
    }
   ],
   "source": [
    "client = PersistentClient(path='catbot/database/chroma')\n",
    "collection = client.get_collection(\"its_all_about_cats\", embedding_function=embedding_function(model_name=\"text-embedding-3-small\"))\n",
    "\n",
    "rag = BasicRAG(collection)\n",
    "\n",
    "question = \"What is the history of cats?\"\n",
    "response = rag.respond(question)\n",
    "\n",
    "print(\"Question: \", question)\n",
    "print(\"Response: \", response['response'])\n",
    "\n",
    "context_titles = [context['metadata']['title'] for context in response['sources']]\n",
    "print(\"Contexts:\")\n",
    "for title in context_titles:\n",
    "    print(\"->\", title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe2575",
   "metadata": {},
   "source": [
    "## Generate Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d5511",
   "metadata": {},
   "source": [
    "### Using a single call prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393a340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following documentation page, generate one clear, specific question that can \n",
    "be answered using information found only on this page.\n",
    "\n",
    "Guidelines:\n",
    "- Keep the question focused and concise (no more than 25 words).\n",
    "- Make it practicalâ€”ask about concepts, actions, or procedures as a real user would.\n",
    "- Avoid asking for lists of steps or comprehensive summaries.\n",
    "- Ask as if you needed a precise answer for a specific task, not a generic explanation.\n",
    "- Avoid overly broad, complex, or multi-part questions.\n",
    "- Do not ask for details not explicitly stated in the document.\n",
    "- Don't use the question you find in the title, come up with something new\n",
    "Example of a good question:\n",
    "- How do I reconcile Mollie transactions with payments and invoices?\n",
    "\n",
    "Documentation Page:\n",
    "{context}\n",
    "\n",
    "Generated question: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33365390",
   "metadata": {},
   "source": [
    "## Using Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers import (\n",
    "    # SingleHopSpecificQuerySynthesizer,\n",
    "    # MultiHopAbstractQuerySynthesizer,\n",
    "    # MultiHopSpecificQuerySynthesizer,\n",
    "    default_query_distribution\n",
    ")\n",
    "query_distribution = default_query_distribution(llm=generator_llm)\n",
    "\n",
    "\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "query_distribution = default_query_distribution(llm=generator_llm)\n",
    "\n",
    "synthetic_ragas = generator.generate_with_langchain_docs(sections, testset_size=5, query_distribution=[query_distribution[2]],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic_ragas = generator.generate_with_langchain_docs(docs_ragas, testset_size=len(docs_ragas))\n",
    "from ragas.testset.synthesizers import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    "    MultiHopAbstractQuerySynthesizer,\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    "    default_query_distribution\n",
    ")\n",
    "query_distribution = default_query_distribution(llm=generator_llm)\n",
    "\n",
    "\n",
    "# synthetic_ragas = generator.generate_with_langchain_docs(docs_ragas, testset_size=3, with_debugging_logs = True, query_distribution=[multihop_query_synthesizer],)\n",
    "# synthetic_ragas = generator.generate_with_langchain_docs(sections, testset_size=2, with_debugging_logs = True, query_distribution=[query_distribution[0]],)\n",
    "synthetic_ragas = generator.generate_with_langchain_docs(sections, testset_size=5, query_distribution=[query_distribution[2]],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73546d03",
   "metadata": {},
   "source": [
    "## Evaluation with Ragas metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c041d3be",
   "metadata": {},
   "source": [
    "Define the LLM and embeddings model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb553715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/bdywk_3j5q90gzyv79gkq3jc0000gp/T/ipykernel_92750/1333563673.py:32: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  generator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c13d9307",
   "metadata": {},
   "source": [
    "### Retrieval: Context Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40222d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    reference = 'some text', #Todo\n",
    "    retrieved_contexts=[context['document'] for context in response['sources']],\n",
    ")\n",
    "\n",
    "context_recall = LLMContextRecall(llm=generator_llm)\n",
    "await context_recall.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eaf400",
   "metadata": {},
   "source": [
    "## Generation: Faithfulness and Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd66766",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "Evaluate the response based on the retrieved context, requires question, response and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53dab947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is the history of cats?\n",
      "Response:  Cats have been domesticated for nearly 10,000 years. The oldest evidence of cats kept as pets is from the Mediterranean island of Cyprus, around 7500 BC. In ancient times, especially in Egypt, people kept cats because they hunted and ate mice and rats. Ancient Egyptians worshipped cats as gods and often mummified them so they could be with their owners \"for all of eternity.\" Cats started becoming pets during the time of the ancient Egyptians. Today, people often keep cats as pets, and some domestic cats live without care from people and are known as \"feral cats\" or \"stray cats.\"\n",
      "Contexts:  2  contexts\n",
      "Faithfulness score:  1.0\n"
     ]
    }
   ],
   "source": [
    "sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    response=response['response'],\n",
    "    retrieved_contexts=[context['document'] for context in response['sources']],\n",
    ")\n",
    "scorer = Faithfulness(llm=generator_llm)\n",
    "\n",
    "print(\"Question: \", sample.user_input)\n",
    "print(\"Response: \", sample.response)\n",
    "print(\"Contexts: \", len(sample.retrieved_contexts), \" contexts\")\n",
    "print(\"Faithfulness score: \", await scorer.single_turn_ascore(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b177afb",
   "metadata": {},
   "source": [
    "### Answer Correctness\n",
    "Evaluate the response based on the ground truth, requires question, response and context and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698372d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2724298216509958"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "reference = 'What is the plural form of \"felis\" in Latin?'\n",
    "\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    response=response['response'],\n",
    "    reference=reference,\n",
    "    retrieved_contexts=[context['document'] for context in response['sources']], #todo - can this be commented out\n",
    ")\n",
    "\n",
    "answer_similarity = AnswerSimilarity(embeddings = generator_embeddings)\n",
    "scorer = AnswerCorrectness(llm=generator_llm, answer_similarity=answer_similarity, weights = [0,1])\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415d2ac",
   "metadata": {},
   "source": [
    "Answer Correctness needs the semantic similarity module, it can be explicitely called as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b6da91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity:  0.5873162152223527\n"
     ]
    }
   ],
   "source": [
    "sample = SingleTurnSample(\n",
    "    response='what is the latin name for cat?',\n",
    "    reference  = 'Translate \"Felis catus\"'\n",
    ")\n",
    "\n",
    "scorer = SemanticSimilarity(embeddings=generator_embeddings)\n",
    "print(\"Semantic Similarity: \", await scorer.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a6afa",
   "metadata": {},
   "source": [
    "## End-to-End: Response Relevancy\n",
    "Generates a set of questions from the response, embedds them, calculates cosine similarity to the user question and averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca747a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7658875076572474)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    response = response['response'],\n",
    "    )\n",
    "\n",
    "scorer = ResponseRelevancy(llm=generator_llm, embeddings=generator_embeddings)\n",
    "\n",
    "print(\"Question: \", sample.user_input)\n",
    "print(\"Response: \", sample.response)\n",
    "print(\"Response Relevancy: \", await scorer.single_turn_ascore(sample))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".catbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
